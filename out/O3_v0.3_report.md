# O3 Function Start Detector v0.3

## Changes vs v0.2
- Added conditional-branch targets into the candidate generator so CFG-heavy O3 builds expose more training/eval points without retraining.
- Extended the prediction post-filter with a jump-table heuristic (examines the next ~48 bytes for JMP/data/large-imm runs) layered atop the existing ≥3-NOP padding gate, plus merge-nearby logic from v0.2.
- Installed Ghidra 11.4.2 headless into `third_party/` with export + mining helpers (`tools/ghidra_export_functions.py`, `scripts/tools_ghidra.sh`, `src/mining/make_hard_negatives.py`).

## Commands
- `source .venv/bin/activate && THRESH=<thr> POST_FILTER=on MODEL_PATH=models/start_detector_v0.1.joblib src/run_batch_predict.sh` for thr ∈ {0.25…0.55}, each saved to `out/summary_thr_<thr>_v03_filtered.tsv`.
- Same sweep with `POST_FILTER=off`, saved to `out/summary_thr_<thr>_v03_raw.tsv`.
- `python - <<'PY' ...` (See shell history) to aggregate macros into `out/macro_v03.tsv`.

## Macro Metrics (v0.3)

| Threshold | Macro P (filtered) | Macro R (filtered) | Macro F1 (filtered) | Macro P (raw) | Macro R (raw) | Macro F1 (raw) |
|-----------|--------------------|--------------------|---------------------|---------------|---------------|----------------|
| 0.25 | 0.739 | 0.906 | 0.813 | 0.739 | 0.906 | 0.813 |
| 0.30 | 0.781 | 0.906 | 0.836 | 0.781 | 0.906 | 0.836 |
| 0.33 | 0.781 | 0.906 | 0.836 | 0.781 | 0.906 | 0.836 |
| 0.35 | 0.781 | 0.906 | 0.836 | 0.781 | 0.906 | 0.836 |
| **0.40** | **0.889** | **0.801** | **0.839** | **0.889** | **0.801** | **0.839** |
| 0.45 | 0.909 | 0.726 | 0.803 | 0.909 | 0.726 | 0.803 |
| 0.55 | 0.958 | 0.587 | 0.724 | 0.958 | 0.587 | 0.724 |

Best threshold (filtered and raw): **0.40** (MacroF1 0.839, MacroR 0.801).

## Per-Binary Metrics @ Threshold 0.40

| Mode | Binary | TP | FP | FN | P | R | F1 | mean_err | median_err |
|------|--------|----|----|----|---|---|----|----------|-------------|
| filtered | hello | 8 | 0 | 2 | 1.000 | 0.800 | 0.889 | 1.0 | 0.0 |
|          | mathlib | 7 | 0 | 1 | 1.000 | 0.875 | 0.933 | 0.0 | 0.0 |
|          | sort | 8 | 4 | 3 | 0.667 | 0.727 | 0.696 | 1.0 | 0.0 |
| raw      | hello | 8 | 0 | 2 | 1.000 | 0.800 | 0.889 | 1.0 | 0.0 |
|          | mathlib | 7 | 0 | 1 | 1.000 | 0.875 | 0.933 | 0.0 | 0.0 |
|          | sort | 8 | 4 | 3 | 0.667 | 0.727 | 0.696 | 1.0 | 0.0 |

## Jump-Table Filter Observations
- On the current three O3 stripped samples, the jt-filter removed **0** candidates (no jump-table islands with branch/data-heavy runs were present). Logs such as `out/logs/run_batch_predict_thr_0.25_v03_filtered_detail.log` show both padding and jt removal counts, all zero.
- Expect this heuristic to engage once binaries include switch/jump-table thunks emitted around alignment sleds; filtering requires `xrefs_in == 0` and a ≥50% concentration of JMP/data/large-imm instructions in the next ~48 bytes.

## Ghidra Hard-Negative Mining
- `scripts/tools_ghidra.sh` + `tools/ghidra_export_functions.py` emitted CSVs under `out/ghidra/`: hello (export log `out/logs/ghidra_hello_stripped.log`), mathlib, sort.
- `src/mining/make_hard_negatives.py --tolerance 8` generated:
  * `out/mining/hello_stripped_hardnegs.json` — 14 negatives
  * `out/mining/mathlib_stripped_hardnegs.json` — 35 negatives
  * `out/mining/sort_stripped_hardnegs.json` — 26 negatives
- These snapshots capture feature vectors for candidates >8 bytes away from any Ghidra start; feed them into future re-training or calibration to demote persistent false positives.

## Interpretation
1. Adding conditional-branch targets slightly increases candidate diversity without perturbing precision/recall on this corpus; the logistic model still prefers true prologues, so macro curves mirror v0.2.
2. The new jt-filter presently acts as a safeguard: it didn’t fire here, but instrumentation confirms counts so we can monitor once padding-heavy switch tables appear.
3. Mean/median start offsets remain ≤1 byte, indicating the widened context + merge-nearby logic still land near symbol boundaries even when filters are enabled.
4. Ghidra hard negatives (14/35/26 per binary) give us concrete feature snapshots for upcoming retrains or calibration sweeps targeting stubborn FPs.

## Next Actions
1. Install Ghidra headless tools and run the planned export → hard-negative mining loop.
2. Expand the O3 corpus with switch-heavy samples to validate the jt-filter and tune the large-immediate detector.
3. Explore augmenting the feature set with per-function entropy or basic-block degree stats to better isolate alignment sleds vs legitimate helpers.
