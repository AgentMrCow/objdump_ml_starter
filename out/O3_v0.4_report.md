# O3 Function Start Detector v0.4

## Commands
- `source .venv/bin/activate && python src/train_start_detector.py --train_glob 'data/build/*/O0/*_sym' --train_glob 'data/build/*/O3/*_sym' --hn_json_glob 'out/mining/*_hardnegs.json' --model_path models/start_detector_v0.4.joblib`
- `source .venv/bin/activate && for thr in 0.25 0.30 0.33 0.35 0.40 0.45 0.55; do THRESH=$thr POST_FILTER=on MODEL_PATH=models/start_detector_v0.4.joblib src/run_batch_predict.sh; cp out/summary.tsv out/summary_thr_${thr}_v04_filtered.tsv; THRESH=$thr POST_FILTER=off MODEL_PATH=models/start_detector_v0.4.joblib src/run_batch_predict.sh; cp out/summary.tsv out/summary_thr_${thr}_v04_raw.tsv; done`
- `source .venv/bin/activate && python tools/aggregate_macros.py --thresholds 0.25 0.30 0.33 0.35 0.40 0.45 0.55 --pattern v03 filtered out/summary_thr_{thr}_v03_filtered.tsv --pattern v03 raw out/summary_thr_{thr}_v03_raw.tsv --pattern v04 filtered out/summary_thr_{thr}_v04_filtered.tsv --pattern v04 raw out/summary_thr_{thr}_v04_raw.tsv --out out/macro_v03_vs_v04.tsv`
- `python - <<'PY' ...` (see shell history) to compare best-threshold predictions vs Ghidra and write `out/ghidra_compare_v04_0.45.tsv`, `notes/ghidra_gaps_v04.md`.

## Key Code Changes
```diff
--- a/src/train_start_detector.py
+++ b/src/train_start_detector.py
@@
-from sklearn.linear_model import LogisticRegression
+from sklearn.linear_model import LogisticRegression
+from sklearn.metrics import log_loss
@@
-def train(train_globs, model_path):
-    items = collect_train_items(train_globs)
-    X_all, y_all = [], []
-    feature_keys = get_feature_keys()
+def add_sample(store, vec, label):
+    key = tuple(vec)
+    existing = store.get(key)
+    if existing is None or label > existing:
+        store[key] = label
+
+
+def load_hard_negative_vectors(hn_globs, feature_keys):
+    vectors = []
+    if not hn_globs:
+        return vectors
+    for pattern in hn_globs:
+        for path in glob.glob(pattern):
+            ...
+
+def train(train_globs, hn_globs, model_path):
+    items = collect_train_items(train_globs)
+    feature_keys = get_feature_keys()
+    samples = {}
+    raw_pos = 0
+    raw_neg = 0
@@
-            feats = featurize_point(instrs, idx)
-            vec = [feats[k] for k in feature_keys]
-            X.append(vec)
-        cand_addrs = [instrs[i]["addr"] for i in cand_idxs]
-        truth = truth_from_file(truth_json)
-        y = label_vector(instrs, cand_addrs, truth)
-        X_all.extend(X)
-        y_all.extend(list(y))
+            feats = featurize_point(instrs, idx)
+            vec = [feats[k] for k in feature_keys]
+            addr = instrs[idx]["addr"]
+            label = 1 if addr in truth_starts else 0
+            raw_pos += label
+            raw_neg += (1 - label)
+            add_sample(samples, vec, label)
+
+    hn_vectors = load_hard_negative_vectors(hn_globs, feature_keys)
+    for vec in hn_vectors:
+        raw_neg += 1
+        add_sample(samples, vec, 0)
@@
-    clf = LogisticRegression(max_iter=1000)
+    clf = LogisticRegression(max_iter=1000)
@@
-    dump({"model": clf, "feature_keys": feature_keys}, model_path)
-    print(f"Trained on {len(y_all)} samples. Saved -> {model_path}")
+    dump({"model": clf, "feature_keys": feature_keys}, model_path)
+    print(f"Raw counts -> pos:{raw_pos} neg:{raw_neg}")
+    print(f"Deduped counts -> pos:{unique_pos} neg:{unique_neg} (total {len(y_all)})")
+    print(f"Training log-loss={loss:.4f} iterations={clf.n_iter_[0] if hasattr(clf, 'n_iter_') else 'n/a'}")
+    print(f"Trained on {len(y_all)} samples. Saved -> {model_path}")
```

```diff
+++ b/tools/aggregate_macros.py
+#!/usr/bin/env python3
+import argparse, csv, statistics
+...
+    ap.add_argument('--pattern', action='append', nargs=3, ...)
+...
+    with open(args.out, 'w') as f:
+        f.write('version\tmode\tthreshold\tMacroP\tMacroR\tMacroF1\n')
+...
```

## Thresholds
- v0.3 filtered best: **0.40** (MacroF1 0.839 / MacroR 0.801)
- v0.4 filtered best: **0.45** (MacroF1 0.840 / MacroR 0.767)
- Raw best thresholds match the filtered selections above.

## Per-Binary Metrics @ v0.4 Filtered Threshold 0.45

| Binary | TP | FP | FN | P | R | F1 | mean_err | median_err |
|--------|----|----|----|---|---|----|----------|-------------|
| hello | 7 | 0 | 3 | 1.000 | 0.700 | 0.824 | 1.1 | 0.0 |
| mathlib | 7 | 0 | 1 | 1.000 | 0.875 | 0.933 | 0.0 | 0.0 |
| sort | 8 | 2 | 3 | 0.800 | 0.727 | 0.762 | 1.0 | 0.0 |

## Macro Table (v0.3 vs v0.4)

| Version | Mode | Threshold | Macro P | Macro R | Macro F1 |
|---------|------|-----------|---------|---------|----------|
| v0.3 | filtered | 0.40 | 0.889 | 0.801 | 0.839 |
| v0.3 | raw | 0.40 | 0.889 | 0.801 | 0.839 |
| v0.4 | filtered | 0.45 | 0.933 | 0.767 | 0.840 |
| v0.4 | raw | 0.45 | 0.933 | 0.767 | 0.840 |

(Full sweep data in `out/macro_v03_vs_v04.tsv`.)

## Ghidra Comparison @ 0.45 Filtered
| File | Agree | Miss | Extra |
|------|-------|------|-------|
| hello_stripped | 7 | 7 | 0 |
| mathlib_stripped | 0 | 14 | 7 |
| sort_stripped | 8 | 9 | 2 |

Notes on first few misses live in `notes/ghidra_gaps_v04.md`.

## Interpretation
1. Deduped + hard-negative training shrinks the dataset to 129 unique vectors and nudges macro precision up at higher thresholds, yielding a slight MacroF1 gain (0.839 → 0.840) despite a recall dip on O3 hello.
2. Mathlib precision stays perfect while sort benefits from two FP drops compared to v0.3 filtered (P 0.800 vs 0.667) thanks to the HN-enforced logistic weights.
3. The new jt-filter still hasn’t fired on these binaries, but mean/median offset telemetry shows starts remain within ≈1 byte for every case.
4. Ghidra agreement highlights the remaining gap: mathlib still misses all 14 Ghidra starts at this threshold; the mined hard negatives give us concrete feature vectors to target in the next retrain.

## Next Steps
1. Expand the O3 corpus (include switch/jump-table heavy samples) to drive the jt-filter and expose more diverse negatives.
2. Experiment with `class_weight="balanced"` or a shallow tree model (e.g., XGBoost) using the new hard negatives to trade precision for recall, especially on mathlib.
3. Introduce a small dev split or cross-validation to tune thresholds automatically and monitor regression vs Ghidra deltas per binary.
